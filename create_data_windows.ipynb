{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f670b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numba import cuda \n",
    "import os\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0832418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run constants.py\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "TIME_STEP_DELTA:int = TIME_STEP - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b16c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some file name editing for timestep\n",
    "\n",
    "\n",
    "f = os.path.basename(X_TRAIN_SINGLE_TIMESTEP_RAW)\n",
    "X_TRAIN_SINGLE_TIMESTEP_RAW = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "f = os.path.basename(X_TEST_SINGLE_TIMESTEP_RAW)\n",
    "X_TEST_SINGLE_TIMESTEP_RAW = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "f = os.path.basename(X_VAL_SINGLE_TIMESTEP_RAW)\n",
    "X_VAL_SINGLE_TIMESTEP_RAW = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "\n",
    "\n",
    "f = os.path.basename(X_TRAIN_INPUT_SAVE_FILE_VEC_SINGLE_TIMESTEP)\n",
    "X_TRAIN_INPUT_SAVE_FILE_VEC_SINGLE_TIMESTEP = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "f = os.path.basename(X_TEST_INPUT_SAVE_FILE_VEC_SINGLE_TIMESTEP)\n",
    "X_TEST_INPUT_SAVE_FILE_VEC_SINGLE_TIMESTEP = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "f = os.path.basename(X_VAL_INPUT_SAVE_FILE_VEC_SINGLE_TIMESTEP)\n",
    "X_VAL_INPUT_SAVE_FILE_VEC_SINGLE_TIMESTEP = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "\n",
    "f = os.path.basename(X_TRAIN_INPUT_SAVE_FILE_VEC_MULTI_TIMESTEP)\n",
    "X_TRAIN_INPUT_SAVE_FILE_VEC_MULTI_TIMESTEP = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "f = os.path.basename(X_TEST_INPUT_SAVE_FILE_VEC_MULTI_TIMESTEP)\n",
    "X_TEST_INPUT_SAVE_FILE_VEC_MULTI_TIMESTEP = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "f = os.path.basename(X_VAL_INPUT_SAVE_FILE_VEC_MULTI_TIMESTEP)\n",
    "X_VAL_INPUT_SAVE_FILE_VEC_MULTI_TIMESTEP = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "\n",
    "f = os.path.basename(Y_TRAIN_INPUT_SAVE_FILE)\n",
    "Y_TRAIN_INPUT_SAVE_FILE = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "f = os.path.basename(Y_TEST_INPUT_SAVE_FILE)\n",
    "Y_TEST_INPUT_SAVE_FILE = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "f = os.path.basename(Y_VAL_INPUT_SAVE_FILE)\n",
    "Y_VAL_INPUT_SAVE_FILE = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "\n",
    "f = os.path.basename(EMBEDDING_MATRIX_SAVE_FILE)\n",
    "EMBEDDING_MATRIX_SAVE_FILE= os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "f = os.path.basename(VOCAB_SAVE_FILE)\n",
    "VOCAB_SAVE_FILE = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) \n",
    "f = os.path.basename(TRAIN_CORPORA)\n",
    "TRAIN_CORPORA = os.path.join(DATA_DIR, os.path.splitext(os.path.basename(f))[0] + \"_\" + str(TIME_STEP) + os.path.splitext(os.path.basename(f))[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d78435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clamp(minimum: int, x: int, maximum: int):\n",
    "    \"\"\"Clamps an integer between a min/max\"\"\"\n",
    "    return max(minimum, min(x, maximum))\n",
    "\n",
    "\n",
    "\n",
    "class WindowGenerator:\n",
    "    \"\"\"\n",
    "    Class to generate timestep'd data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_width: int, output_width: int, save_windows: bool):\n",
    "        \"\"\"Init Parmas\n",
    "        Args:\n",
    "            input_width (int): The timesteps forming the input sequence\n",
    "            output_width (int): The timesteps forming the output sequence\n",
    "        \"\"\"\n",
    "        self.input_width: int = input_width\n",
    "        self.output_width: int = output_width\n",
    "        self.total_window_size: int = input_width + output_width\n",
    "        self.minimum_day_of_year: int = 0\n",
    "        self.maximum_day_of_year: int = 365\n",
    "        self.save_windows: bool = save_windows\n",
    "\n",
    "    def window_datafile(\n",
    "        self, data: pandas.DataFrame\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        sequence: list = []\n",
    "        labels: list = []\n",
    "        for index, row in data.iterrows():\n",
    "            for column in row.index[row.notnull()]:\n",
    "                column = int(column)\n",
    "                lower_bound = clamp(\n",
    "                    self.minimum_day_of_year,\n",
    "                    column - self.input_width,\n",
    "                    self.maximum_day_of_year,\n",
    "                )\n",
    "\n",
    "                upper_bound = clamp(\n",
    "                    0,\n",
    "                    column + self.input_width,\n",
    "                    self.maximum_day_of_year,\n",
    "                )\n",
    "\n",
    "                visit_index = column \n",
    "\n",
    "                input_sequence = data.iloc[index, lower_bound + 1 : visit_index]\n",
    "                input_sequence = input_sequence.to_numpy()\n",
    "                output_sequence = data.iloc[index, visit_index : upper_bound + 1]\n",
    "                output_sequence = output_sequence.to_numpy()\n",
    "                if len(input_sequence) < self.input_width:\n",
    "                    input_sequence = self._pad_timeseries(sequence=input_sequence)\n",
    "                if len(input_sequence) != TIME_STEP:\n",
    "                    raise ValueError(\n",
    "                        f\"Input sequence has incorrect length :{len(input_sequence)} when compared to timestep window: {TIME_STEP -1}\"\n",
    "                    )\n",
    "                sequence.append(input_sequence)\n",
    "\n",
    "                label = self._categorize_output_sequence(\n",
    "                    output_sequence=output_sequence\n",
    "                )\n",
    "                labels.append(label)\n",
    "        if self.save_windows:\n",
    "            self.save_frames(output_labels=np.array(labels), input_sequence=sequence)\n",
    "\n",
    "        return sequence, np.array(labels)\n",
    "\n",
    "    def _pad_timeseries(self, sequence):\n",
    "        pad_nan_delta = self.input_width - len(sequence)\n",
    "        if pad_nan_delta > 0:\n",
    "            sequence = np.pad(\n",
    "                sequence,\n",
    "                (pad_nan_delta, 0),\n",
    "                \"constant\",\n",
    "                constant_values=EMPTY_TIMESTEP_TOKEN,\n",
    "            )\n",
    "        return sequence\n",
    "\n",
    "    def save_frames(self, output_labels, input_sequence):\n",
    "        print(\"------Saving windows for reuse ------\")\n",
    "        with open(REARRANGED_INPUT_WINDOWED_DATA_FILEPATH, \"wb\") as f:\n",
    "            pickle.dump(input_sequence, f)\n",
    "        with open(REARRANGED_INPUT_WINDOWED_LABEL_FILEPATH, \"wb\") as f:\n",
    "            pickle.dump(output_labels, f)\n",
    "\n",
    "    def _categorize_output_sequence(self, output_sequence: pandas.DataFrame) -> bool:\n",
    "        \"\"\"Categorise output sequence to binary\n",
    "        Classification is based on if output sequence is not null in the output width\n",
    "        Args:\n",
    "            output_sequence (pandas.DataFrame): Sequence to classify\n",
    "        Returns:\n",
    "            bool: 0 = no revisit, 1 = revisit\n",
    "        \"\"\"\n",
    "        try:\n",
    "            np.isnan(np.sum(output_sequence))\n",
    "            return 0\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "\n",
    "def generate_windows(time_series_df):\n",
    "\n",
    "    w1 = WindowGenerator(\n",
    "                input_width=TIME_STEP, output_width=TIME_STEP, save_windows=True\n",
    "            )\n",
    "    loaded_dataset, loaded_labels = w1.window_datafile(time_series_df)\n",
    "   \n",
    "    print(\"------ Windowed Data Loaded ------\")\n",
    "    return loaded_dataset, loaded_labels\n",
    "\n",
    "\n",
    "def vectorize_data_multi_timestep(text_vectorization, loaded_dataset):\n",
    "    arr = numpy.array(loaded_dataset)\n",
    "    arr[pd.isnull(arr)] = EMPTY_TIMESTEP_TOKEN\n",
    "    input_samples = []\n",
    "    for _, item in enumerate(\n",
    "        tqdm.tqdm(arr, desc=\"Vectoring multi timestep\"),\n",
    "    ):\n",
    "        time_seq = []\n",
    "        for _, timestep in enumerate(item):\n",
    "            time_seq.append(text_vectorization(timestep))\n",
    "        input_samples.append(time_seq)\n",
    "    test = numpy.array(input_samples)\n",
    "    return test\n",
    "\n",
    "def embed_vectors(text_vectorization):\n",
    "    embeddings_index = {}\n",
    "\n",
    "    f = open(GLOVE_100D_FILEPATH)\n",
    "    for line in tqdm.tqdm(f, ncols=100, desc=\"Loading Glove Embeddings.\"):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = numpy.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print(f\"Found {len(embeddings_index)} word vectors.\")\n",
    "\n",
    "    vocabulary = text_vectorization.get_vocabulary()\n",
    "    word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    embedding_matrix = numpy.zeros((MAX_VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in tqdm.tqdm(word_index.items(), desc=\"Embedding Matrix.\"):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bfdf8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Saving windows for reuse ------\n",
      "------ Windowed Data Loaded ------\n"
     ]
    }
   ],
   "source": [
    "time_series_df = pd.read_csv(REARRANGED_DATA_FILEPATH)\n",
    "time_series_df = time_series_df.iloc[: , 1:]\n",
    "loaded_ds, loaded_labels = generate_windows(time_series_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b837e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(loaded_ds)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_test, loaded_labels, test_size=TEST_TRAIN_SPLIT, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATION_SPLIT, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8dfe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  ((2465, 90), (2465,)) \n",
      "Validation data shape:  ((274, 90), (274,)) \n",
      "Testing data shape:  ((685, 90), (685,)) \n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data shape:  {X_train.shape, y_train.shape} \")\n",
    "print(f\"Validation data shape:  {X_val.shape, y_val.shape} \")\n",
    "print(f\"Testing data shape:  {X_test.shape, y_test.shape} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d1d855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 18:36:49.917254: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-31 18:36:50.803361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 113 MB memory:  -> device: 0, name: GeForce RTX 2070, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2022-07-31 18:36:50.804207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6414 MB memory:  -> device: 1, name: GeForce RTX 2070, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2022-07-31 18:36:50.809948: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 113.31M (118816768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "def create_textvectorisation(lst):\n",
    "    text_vectorization: TextVectorization = TextVectorization(\n",
    "        output_mode=\"int\",\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=MAX_VOCAB_SIZE,\n",
    "        output_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    )\n",
    "    text_vectorization.adapt(lst)\n",
    "    return text_vectorization\n",
    "\n",
    "\n",
    "#Shave off the training corpora for fine tuning glove embeddings with it\n",
    "train_corpora = X_train[TIME_STEP_DELTA].str.split()\n",
    "train_corpora = train_corpora.tolist()\n",
    "flat_list_train_corpora = [x for xs in train_corpora for x in xs]\n",
    "flat_list_train_corpora = list(set(flat_list_train_corpora))\n",
    "flat_list_train_corpora\n",
    "\n",
    "text_vectorization = create_textvectorisation(flat_list_train_corpora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a6a6bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectoring multi timestep: 100%|██████████| 2465/2465 [17:58<00:00,  2.29it/s]\n",
      "Vectoring multi timestep: 100%|██████████| 685/685 [05:00<00:00,  2.28it/s]\n",
      "Vectoring multi timestep: 100%|██████████| 274/274 [01:59<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_df(df):\n",
    "    no_uppercase = df.apply(lambda x: x.astype(str).str.lower()) \n",
    "    no_html = no_uppercase.replace(r'<[^<>]*>', '', regex=True)\n",
    "    no_punctuation = no_html.replace(r'[^\\w]', ' ', regex=True)\n",
    "    no_digits = no_punctuation.replace(r'\\w*\\d\\w*', ' ', regex=True)\n",
    "    return no_digits\n",
    "\n",
    "x_train = clean_df(X_train)\n",
    "x_val = clean_df(X_val)\n",
    "x_test = clean_df(X_test)\n",
    "\n",
    "X_train.fillna(EMPTY_TIMESTEP_TOKEN, inplace=True)\n",
    "X_test.fillna(EMPTY_TIMESTEP_TOKEN, inplace=True)\n",
    "X_val.fillna(EMPTY_TIMESTEP_TOKEN, inplace=True)\n",
    "\n",
    "# Multistep timeframes\n",
    "x_train_multistep_vec = vectorize_data_multi_timestep(text_vectorization, X_train)\n",
    "x_test_multistep_vec = vectorize_data_multi_timestep(text_vectorization, X_test)\n",
    "x_val_multistep_vec = vectorize_data_multi_timestep(text_vectorization, X_val)\n",
    "\n",
    "# Single step timeframes\n",
    "x_test_one_timestep_vec = text_vectorization(numpy.array(X_test[TIME_STEP_DELTA].to_list()))\n",
    "x_train_one_timestep_vec = text_vectorization(numpy.array(X_train[TIME_STEP_DELTA].to_list()))\n",
    "x_val_one_timestep_vec = text_vectorization(numpy.array(X_val[TIME_STEP_DELTA].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d7bf91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Glove Embeddings.: 400000it [00:05, 71998.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Matrix.: 100%|██████████| 10000/10000 [00:00<00:00, 698212.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove*.zip\n",
    "\n",
    "embedding_matrix = embed_vectors(text_vectorization)\n",
    "vocab = text_vectorization.get_vocabulary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb23c67",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the variables!\n",
    "\n",
    "\n",
    "with open(X_TRAIN_SINGLE_TIMESTEP_RAW, \"wb\") as f:\n",
    "        pickle.dump(numpy.array(X_train[TIME_STEP_DELTA].to_list()), f)\n",
    "with open(X_TEST_SINGLE_TIMESTEP_RAW, \"wb\") as f:\n",
    "        pickle.dump(numpy.array(X_test[TIME_STEP_DELTA].to_list()), f)\n",
    "with open(X_VAL_SINGLE_TIMESTEP_RAW, \"wb\") as f:\n",
    "        pickle.dump(numpy.array(X_val[TIME_STEP_DELTA].to_list()), f)\n",
    "\n",
    "with open(X_TRAIN_INPUT_SAVE_FILE_VEC_SINGLE_TIMESTEP, \"wb\") as f:\n",
    "        pickle.dump(x_train_one_timestep_vec, f)\n",
    "with open(X_TEST_INPUT_SAVE_FILE_VEC_SINGLE_TIMESTEP, \"wb\") as f:\n",
    "        pickle.dump(x_test_one_timestep_vec, f)\n",
    "with open(X_VAL_INPUT_SAVE_FILE_VEC_SINGLE_TIMESTEP, \"wb\") as f:\n",
    "        pickle.dump(x_val_one_timestep_vec, f)\n",
    "\n",
    "with open(X_TRAIN_INPUT_SAVE_FILE_VEC_MULTI_TIMESTEP, \"wb\") as f:\n",
    "        pickle.dump(x_train_multistep_vec, f)\n",
    "with open(X_TEST_INPUT_SAVE_FILE_VEC_MULTI_TIMESTEP, \"wb\") as f:\n",
    "        pickle.dump(x_test_multistep_vec, f)\n",
    "with open(X_VAL_INPUT_SAVE_FILE_VEC_MULTI_TIMESTEP, \"wb\") as f:\n",
    "        pickle.dump(x_val_multistep_vec, f)\n",
    "\n",
    "\n",
    "with open(Y_TRAIN_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_train, f)\n",
    "with open(Y_TEST_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_test, f)\n",
    "with open(Y_VAL_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_val, f)\n",
    "\n",
    "with open(EMBEDDING_MATRIX_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(embedding_matrix, f)\n",
    "with open(VOCAB_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n",
    "with open(TRAIN_CORPORA, \"wb\") as f:\n",
    "        pickle.dump(flat_list_train_corpora, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6f07f10bbd904384622e2c81da346bac6398da26490ea76cc729ffb1c8c49fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
